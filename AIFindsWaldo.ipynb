{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from matplotlib.pyplot import imshow\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img, save_img\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONSTANTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pics_waldo_dir = 'Hey-Waldo\\\\64-waldo'\n",
    "pics_notwaldo_dir = 'Hey-Waldo\\\\64-notwaldo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOADING ORIGINAL WALDO'S PICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_waldo = list()\n",
    "for filename in os.listdir(pics_waldo_dir):\n",
    "    x_waldo_temp = img_to_array(load_img(pics_waldo_dir+'\\\\'+filename))\n",
    "    x_waldo.append(x_waldo_temp.reshape((1,) + x_waldo_temp.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOADING ALL WALDO'S PICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_waldo = list()\n",
    "for filename in os.listdir(pics_waldo_dir):\n",
    "    x_waldo.append(np.asarray(Image.open(pics_waldo_dir+'\\\\'+filename)))\n",
    "y_waldo = np.ones(len(x_waldo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOADING NOT WALDO'S PICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_notwaldo = list()\n",
    "for filename in os.listdir(pics_notwaldo_dir):\n",
    "    aux_img = Image.open(pics_notwaldo_dir+'\\\\'+filename)\n",
    "    aux_img = aux_img.convert('RGB')\n",
    "    x_notwaldo.append(np.asarray(aux_img))\n",
    "y_notwaldo = np.zeros(len(x_notwaldo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x_waldo + x_notwaldo)\n",
    "Y = np.array(list(y_waldo) + list(y_notwaldo))[:int(len(Y)/10)]\n",
    "X = X.reshape(len(X), 64, 64, 3)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#for i in range(len(y_train)):\n",
    "#    if y_train[i] == 1:\n",
    "#        imshow(X_train[i])\n",
    "#        break\n",
    "#y_train = to_categorical(y_train, num_classes = 2)\n",
    "#y_test = to_categorical(y_test, num_classes = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "744/744 [==============================] - 35s 47ms/step - loss: 0.0179 - auc: 0.0000e+00\n",
      "Epoch 2/3\n",
      "744/744 [==============================] - 34s 46ms/step - loss: 0.0000e+00 - auc: 0.0000e+00\n",
      "Epoch 3/3\n",
      "744/744 [==============================] - 34s 46ms/step - loss: 0.0000e+00 - auc: 0.0000e+00\n",
      "186/186 [==============================] - 2s 13ms/step - loss: 1536.9050 - auc: 0.5000\n",
      "Model evaluation  [1536.905029296875, 0.5]\n",
      "Epoch 1/3\n",
      "744/744 [==============================] - 34s 45ms/step - loss: 1.0566 - auc: 0.8060\n",
      "Epoch 2/3\n",
      "744/744 [==============================] - 34s 45ms/step - loss: 0.1389 - auc: 0.9367\n",
      "Epoch 3/3\n",
      "744/744 [==============================] - 33s 45ms/step - loss: 0.1143 - auc: 0.9604\n",
      "186/186 [==============================] - 2s 13ms/step - loss: 0.0418 - auc: 0.0000e+00: 0s - loss: 0.0413 - auc: 0.0000e+0\n",
      "Model evaluation  [0.041750382632017136, 0.0]\n",
      "Epoch 1/3\n",
      "744/744 [==============================] - 34s 46ms/step - loss: 1.0774 - auc: 0.8208\n",
      "Epoch 2/3\n",
      "744/744 [==============================] - 34s 46ms/step - loss: 0.1424 - auc: 0.9316\n",
      "Epoch 3/3\n",
      "744/744 [==============================] - 33s 44ms/step - loss: 0.1029 - auc: 0.9652: 0s - loss: 0.1034 - au\n",
      "186/186 [==============================] - 2s 12ms/step - loss: 0.0196 - auc: 0.0000e+00\n",
      "Model evaluation  [0.01957375928759575, 0.0]\n",
      "Epoch 1/3\n",
      "744/744 [==============================] - 33s 44ms/step - loss: 1.0704 - auc: 0.7840\n",
      "Epoch 2/3\n",
      "744/744 [==============================] - 33s 44ms/step - loss: 0.1328 - auc: 0.9399\n",
      "Epoch 3/3\n",
      "744/744 [==============================] - 33s 45ms/step - loss: 0.0990 - auc: 0.9667 \n",
      "186/186 [==============================] - 2s 13ms/step - loss: 0.0527 - auc: 0.0000e+00A\n",
      "Model evaluation  [0.05273415893316269, 0.0]\n",
      "Epoch 1/3\n",
      "744/744 [==============================] - 33s 45ms/step - loss: 2.1607 - auc: 0.7524 0s - loss: 2.1812 - auc: 0\n",
      "Epoch 2/3\n",
      "744/744 [==============================] - 33s 45ms/step - loss: 0.1699 - auc: 0.8866\n",
      "Epoch 3/3\n",
      "744/744 [==============================] - 33s 45ms/step - loss: 0.1329 - auc: 0.9399 1s - loss:\n",
      "186/186 [==============================] - 2s 13ms/step - loss: 0.0203 - auc: 0.0000e+00\n",
      "Model evaluation  [0.020302385091781616, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#model = Sequential()\n",
    "#model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(64,64,3)))\n",
    "#model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    \n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv2D(filters=32, kernel_size=(2,2), strides=(1,1),padding='same',input_shape=(64,64,3),data_format='channels_last'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "    cnn.add(Conv2D(filters=64,kernel_size=(2,2),strides=(1,1),padding='valid'))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,2),strides=2))\n",
    "    cnn.add(Flatten())        \n",
    "    cnn.add(Dense(64))\n",
    "    cnn.add(Activation('relu'))\n",
    "    cnn.add(Dropout(0.25))\n",
    "    cnn.add(Dense(1))\n",
    "    cnn.add(Activation('sigmoid'))\n",
    "    cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "    \n",
    "    cnn.fit(x_train, y_train, epochs=3)\n",
    "    \n",
    "    print('Model evaluation ',cnn.evaluate(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "744/744 [==============================] - 36s 48ms/step - loss: 0.7968 - auc: 0.7949 - val_loss: 0.1724 - val_auc: 0.9173\n",
      "Epoch 2/6\n",
      "744/744 [==============================] - 35s 47ms/step - loss: 0.1145 - auc: 0.9363 - val_loss: 0.0960 - val_auc: 0.9670\n",
      "Epoch 3/6\n",
      "744/744 [==============================] - 36s 48ms/step - loss: 0.0823 - auc: 0.9651 - val_loss: 0.0749 - val_auc: 0.9816\n",
      "Epoch 4/6\n",
      "744/744 [==============================] - 35s 47ms/step - loss: 0.0662 - auc: 0.9758 - val_loss: 0.0813 - val_auc: 0.9725\n",
      "Epoch 5/6\n",
      "744/744 [==============================] - 35s 48ms/step - loss: 0.0497 - auc: 0.9865 - val_loss: 0.0958 - val_auc: 0.9641\n",
      "Epoch 6/6\n",
      "744/744 [==============================] - 36s 48ms/step - loss: 0.0399 - auc: 0.9913 - val_loss: 0.0547 - val_auc: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b72e67cf08>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(path, input, height, width):\n",
    "    im = Image.open(input)\n",
    "    imgwidth, imgheight = im.size\n",
    "    for i in range(0,imgheight,int(height/2)):\n",
    "        for j in range(0,imgwidth,int(width/2)):\n",
    "            box = (j, i, j+width, i+height)\n",
    "            a = im.crop(box)\n",
    "            try:\n",
    "                o = a.crop((0,0,height, width))\n",
    "                pred_arr = np.asarray(o).reshape(1,64,64,3)\n",
    "                pred = cnn.predict(pred_arr)\n",
    "                if pred[0] > 0.9:\n",
    "                    print(pred)\n",
    "                    #a.show()\n",
    "                \n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop('C:\\\\Users\\\\Bruno\\\\Downloads\\\\Projetos\\\\AIFindsWaldo', '2.jpg', 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13913803]]\n",
      "[[0.02654611]]\n",
      "[[1.6730268e-05]]\n",
      "[[0.21566589]]\n"
     ]
    }
   ],
   "source": [
    "print(cnn.predict(np.asarray(Image.open('3_8_10.jpg')).reshape(1,64,64,3)))\n",
    "print(cnn.predict(np.asarray(Image.open('3_8_11.jpg')).reshape(1,64,64,3)))\n",
    "print(cnn.predict(np.asarray(Image.open('3_8_12.jpg')).reshape(1,64,64,3)))\n",
    "print(cnn.predict(np.asarray(Image.open('20-3.jpg')).reshape(1,64,64,3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
